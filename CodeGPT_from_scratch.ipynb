{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPboHoLpb/uuhwkMgQjIDZd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpandu/CodeGenGPT/blob/main/CodeGPT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training GPT model is done using following steps.\n",
        "1. Load and prepare dataset for tokenizer training.\n",
        "2. Train BPE tokenizer from scratch.\n",
        "3. Prepare tensorflow dataset with generator.\n",
        "4. Create layer and model classes for GPT\n",
        "\n",
        "\n",
        "\n",
        "*   We will use tokenizers library from HuggingFace to train tokenizer from scratch.\n",
        "*   We will also use datasets to load the python \"code_search_net\" dataset. It has ~410k of training records. If we load the load at once we will run out of RAM, so we will take advantage streaming the batches.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0G0cCjYV_-we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "xS4gDfBKzOSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66759d6-c864-4161-b4cf-07bb69d2f6da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub<0.17,>=0.16.4 (from tokenizers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.7.22)\n",
            "Installing collected packages: huggingface_hub, tokenizers\n",
            "Successfully installed huggingface_hub-0.16.4 tokenizers-0.14.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rHXniZJHcUYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "kg8MrYtYNyZ9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Load the \"code_search_net\" adn we can check details of the dataset like below.\n",
        "\n"
      ],
      "metadata": {
        "id": "_mXJ1E_-_jlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# load code dataset\n",
        "raw_dataset = load_dataset(\"code_search_net\", \"python\")\n",
        "raw_dataset"
      ],
      "metadata": {
        "id": "G5lDmsDzXyd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FNmrJRXKFin1",
        "outputId": "9f495865-558f-4681-90d8-f2aa38cccba5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
              "        num_rows: 412178\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
              "        num_rows: 22176\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
              "        num_rows: 23107\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Create a generator to load the data in batches for training tokenizer.\n",
        "\n",
        "   >  Loading all the data at once may cause out of memory error.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hKUjGJprBp22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "    dataset = raw_dataset[\"train\"]\n",
        "    for start_idx in range(0, len(dataset), 1000):\n",
        "        samples = dataset[start_idx : start_idx + 1000]\n",
        "        yield samples[\"whole_func_string\"]\n"
      ],
      "metadata": {
        "id": "z2N_vWvFeKwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if we are able to iterate over the dataset.\n",
        "iterator = iter(get_training_corpus())\n",
        "next(iterator)[0]"
      ],
      "metadata": {
        "id": "j62xMmKOv1sl",
        "outputId": "857a3e39-33e3-42b4-d145-ffe9b363d0d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def export_ruptures_csv(ekey, dstore):\\n    \"\"\"\\n    :param ekey: export key, i.e. a pair (datastore key, fmt)\\n    :param dstore: datastore object\\n    \"\"\"\\n    oq = dstore[\\'oqparam\\']\\n    if \\'scenario\\' in oq.calculation_mode:\\n        return []\\n    dest = dstore.export_path(\\'ruptures.csv\\')\\n    header = (\\'rupid multiplicity mag centroid_lon centroid_lat \\'\\n              \\'centroid_depth trt strike dip rake boundary\\').split()\\n    rows = []\\n    for rgetter in gen_rupture_getters(dstore):\\n        rups = rgetter.get_ruptures()\\n        rup_data = calc.RuptureData(rgetter.trt, rgetter.rlzs_by_gsim)\\n        for r in rup_data.to_array(rups):\\n            rows.append(\\n                (r[\\'rup_id\\'], r[\\'multiplicity\\'], r[\\'mag\\'],\\n                 r[\\'lon\\'], r[\\'lat\\'], r[\\'depth\\'],\\n                 rgetter.trt, r[\\'strike\\'], r[\\'dip\\'], r[\\'rake\\'],\\n                 r[\\'boundary\\']))\\n    rows.sort()  # by rupture serial\\n    comment = \\'investigation_time=%s, ses_per_logic_tree_path=%s\\' % (\\n        oq.investigation_time, oq.ses_per_logic_tree_path)\\n    writers.write_csv(dest, rows, header=header, sep=\\'\\\\t\\', comment=comment)\\n    return [dest]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization:\n",
        "\n",
        "\n",
        "\n",
        "*   Subword Tokenization : Keep frequent words and break rearer words into subwords\n",
        "*   A statastical Alogrothm learns how to do this based on corpus.\n",
        "\n",
        "> Ex: Listeria ---> \"more\" , \"over\"\n",
        "\n",
        "> \"more\" and \"over\" are likely to be more frequent than moreover\n",
        "\n",
        "\n",
        "*   Tokenization has better chance of handling OOV words while decreasing the size of the overall dictionary.   \n",
        "\n",
        "* We will use BPE(Byte Pair Encoding) to train tokenizer on \"code_search_net\" python Dataset.\n",
        "*   For more information on BPE can be found here. https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt\n"
      ],
      "metadata": {
        "id": "VfVPzNJpCzhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "\n",
        "# \"<|endoftext|>\" will used to stop the sequence generation during inference. This is also\n",
        "#  a way telling GPT to learn to about the end of the sequence\n",
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "\n",
        "#Train the tokenizer using BPE trainer, loads the data in batches\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ],
      "metadata": {
        "id": "JuniW9ce_UTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*    Check if we are able to tokenize and encode the data using trained BPE tokenizer\n",
        "*   Actual maximum sequence length MAX_SEQ_LENGTH = 256, we will add one to 256 so that last sample will dropped for the inputs and first sample will be dropped from the outputs. This makes model to see only previous samples to predict next sample.\n",
        "\n",
        "        For example:\n",
        "                 input  :     295    4354   63    72      6035   63\n",
        "                 output :     4354   63     72    6035    63     3170\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1m7xSmfCE6wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = '''def add_numbers(a, b):\n",
        "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
        "    return a + b'''\n",
        "\n",
        "tokenizer.enable_padding(length=257, pad_id = 0, pad_token = \"<|endoftext|>\")\n",
        "tokenizer.enable_truncation(max_length=257)\n",
        "encoding = tokenizer.encode(raw_dataset[\"train\"][1][\"whole_func_string\"])\n",
        "print(encoding.ids)\n",
        "print(encoding.attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLk7DRb2J0QU",
        "outputId": "87ba9c56-c695-45b2-ea0e-78b5599ce84c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[295, 4354, 63, 72, 6035, 63, 3170, 8, 368, 12, 1761, 12, 2840, 4475, 12, 960, 12, 3184, 274, 232, 290, 232, 14210, 256, 9081, 6778, 311, 256, 868, 2899, 1597, 1202, 8446, 14, 312, 310, 394, 521, 26, 819, 63, 369, 350, 4354, 63, 369, 232, 310, 394, 1761, 26, 455, 311, 256, 10245, 480, 232, 310, 394, 2840, 4475, 26, 2840, 2303, 232, 310, 394, 960, 26, 231, 11512, 960, 311, 1719, 366, 923, 63, 3112, 232, 310, 394, 3184, 26, 3184, 292, 811, 442, 1608, 311, 256, 10245, 8446, 480, 232, 290, 232, 12493, 233, 4705, 14, 11080, 63, 7194, 8, 2833, 4475, 12, 960, 9, 232, 2968, 501, 14, 1025, 63, 3170, 8, 2603, 12, 12493, 12, 3184, 29, 2124, 9, 232, 302, 404, 2603, 61, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Generator to prepare inputs and outputs in the batches.\n",
        "*   Inputs and Outputs will have sequences ids encoded from the tokenizer.\n",
        "\n"
      ],
      "metadata": {
        "id": "LL0l-YnLHSJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "def generate_train_data():\n",
        "  decoder_inputs = []\n",
        "  decoder_targets = []\n",
        "  dataset = raw_dataset[\"train\"]\n",
        "  for start_idx in range(0, len(dataset), batch_size):\n",
        "      samples = dataset[start_idx : start_idx + batch_size]\n",
        "      seqs = tokenizer.encode_batch(samples[\"whole_func_string\"])\n",
        "      decoder_inputs = [seq.ids[:-1] for seq in seqs] # Drop the last token in the sentence.\n",
        "      decoder_targets = [seq.ids[1:] for seq in seqs]  # Drop the first token in the sentence.\n",
        "      yield decoder_inputs, decoder_targets\n"
      ],
      "metadata": {
        "id": "iRqM-3Bn2KTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf_dataset = raw_dataset[\"train\"].to_tf_dataset(batch_size = 100, columns = ['whole_func_string'])\n",
        "#tf_dataset"
      ],
      "metadata": {
        "id": "9NBwGTRI4mFr",
        "outputId": "38736a9a-ba65-498d-ef1a-6cc81e846d1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(generate_train_data())\n",
        "decoder_inputs, decoder_targets = next(iterator)\n",
        "print(decoder_inputs[80])\n"
      ],
      "metadata": {
        "id": "7vvqGYjE2-y4",
        "outputId": "c7eb99f5-d319-432d-b45c-28a4cf0fa9b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[295, 627, 52, 18, 48, 13069, 8, 1317, 274, 232, 290, 232, 10197, 231, 19536, 238, 7287, 311, 231, 868, 7335, 3995, 14, 312, 310, 394, 11348, 26, 310, 692, 2310, 4535, 407, 5595, 64, 232, 310, 780, 26, 310, 692, 2310, 46, 395, 238, 48, 13069, 64, 312, 20346, 301, 497, 23110, 8516, 3097, 232, 572, 6313, 14, 77, 659, 1651, 1193, 3883, 14, 2936, 73, 14, 240, 299, 72, 320, 14, 11016, 15, 23701, 15, 338, 619, 12012, 15, 15678, 2561, 15, 15678, 2561, 14, 1825, 7564, 232, 4547, 585, 8898, 89, 627, 889, 65, 336, 350, 690, 279, 347, 4512, 12012, 14, 232, 290, 232, 308, 68, 12, 486, 9, 233, 635, 14, 5908, 14, 12657, 8, 1317, 14, 1317, 9, 232, 523, 233, 635, 14, 783, 929, 68, 59, 17, 547, 270, 59, 16, 547, 270, 59, 18, 4211, 232, 641, 233, 635, 14, 783, 5773, 86, 59, 17, 12, 396, 547, 415, 86, 59, 17, 12, 443, 547, 415, 86, 59, 17, 12, 554, 4936, 1797, 404, 86, 59, 18, 12, 396, 547, 415, 86, 59, 18, 12, 443, 547, 415, 86, 59, 18, 12, 554, 4936, 1797, 4866, 86, 59, 16, 12, 396, 547, 486, 59, 16, 12, 443, 547, 486, 59, 16, 12, 554, 23779, 232, 410, 5309, 233, 523, 14, 10535, 323, 232, 410, 6352, 233, 523, 14, 13410, 323, 232, 393, 37, 233, 308, 54, 1853, 410, 5309, 61, 382, 641, 1853, 410, 6352, 535, 823, 635, 14, 3429, 8, 18, 14, 16, 9, 232, 9893, 233]\n",
            "[295, 627, 52, 18, 48, 13069, 8, 1317, 274, 232, 290, 232, 10197, 231, 19536, 238, 7287, 311, 231, 868, 7335, 3995, 14, 312, 310, 394, 11348, 26, 310, 692, 2310, 4535, 407, 5595, 64, 232, 310, 780, 26, 310, 692, 2310, 46, 395, 238, 48, 13069, 64, 312, 20346, 301, 497, 23110, 8516, 3097, 232, 572, 6313, 14, 77, 659, 1651, 1193, 3883, 14, 2936, 73, 14, 240, 299, 72, 320, 14, 11016, 15, 23701, 15, 338, 619, 12012, 15, 15678, 2561, 15, 15678, 2561, 14, 1825, 7564, 232, 4547, 585, 8898, 89, 627, 889, 65, 336, 350, 690, 279, 347, 4512, 12012, 14, 232, 290, 232, 308, 68, 12, 486, 9, 233, 635, 14, 5908, 14, 12657, 8, 1317, 14, 1317, 9, 232, 523, 233, 635, 14, 783, 929, 68, 59, 17, 547, 270, 59, 16, 547, 270, 59, 18, 4211, 232, 641, 233, 635, 14, 783, 5773, 86, 59, 17, 12, 396, 547, 415, 86, 59, 17, 12, 443, 547, 415, 86, 59, 17, 12, 554, 4936, 1797, 404, 86, 59, 18, 12, 396, 547, 415, 86, 59, 18, 12, 443, 547, 415, 86, 59, 18, 12, 554, 4936, 1797, 4866, 86, 59, 16, 12, 396, 547, 486, 59, 16, 12, 443, 547, 486, 59, 16, 12, 554, 23779, 232, 410, 5309, 233, 523, 14, 10535, 323, 232, 410, 6352, 233, 523, 14, 13410, 323, 232, 393, 37, 233, 308, 54, 1853, 410, 5309, 61, 382, 641, 1853, 410, 6352, 535, 823, 635, 14, 3429, 8, 18, 14, 16, 9, 232, 9893, 233]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf_dataset = tf.data.Dataset.from_tensor_slices((decoder_inputs, decoder_targets)).batch(10, drop_remainder=True)\n",
        "\n",
        "tf_dataset = tf.data.Dataset.from_generator(generate_train_data, output_types=(tf.int32, tf.int32), output_shapes=(tf.TensorShape([50,256]), tf.TensorShape([50,256])))\n",
        "\n",
        "# checks to see if data is loading properly.\n",
        "iterator = iter(tf_dataset)\n",
        "ins, outs = next(iterator)\n",
        "print(ins.shape)\n",
        "next(iterator)[0]"
      ],
      "metadata": {
        "id": "Ysecpmr7uW_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zJuJA3ryJv53",
        "outputId": "c8640795-67d2-4120-d5c0-85a951f94236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 256), dtype=int32, numpy=\n",
              "array([[ 295,  549,   63, ...,  357, 3665,   19],\n",
              "       [ 295,  409, 8485, ...,    0,    0,    0],\n",
              "       [ 295,  409,  322, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [ 295, 1339,    8, ...,    0,    0,    0],\n",
              "       [ 295, 2274,   63, ..., 3921,   63,   66],\n",
              "       [ 295,  549,   63, ...,    0,    0,    0]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Head Attention\n",
        "\n",
        "\n",
        "\n",
        "*   Each Attention head performs Scaled Dot Product Self-Attention operation where given Keys, Query and Values, the return matrix of values given by below operation.\n",
        "\n",
        "        Attention(Q,K,V) = softmax((Q*Transpose(K))/sqrt(d))*V\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WROQaaygJlOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  key_dims = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  scaled_scores = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(key_dims)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
        "\n",
        "  softmax = tf.keras.layers.Softmax()\n",
        "  weights = softmax(scaled_scores)\n",
        "  return tf.matmul(weights, value), weights\n",
        "\n"
      ],
      "metadata": {
        "id": "Q51xPbdZKdc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating queries, keys, and values for multiple heads.**\n",
        "\n",
        "> Now that we have a way to calculate self-attention, let's actually generate the input queries, keys, and values for multiple heads.\n",
        "\n",
        ">  each attention head had its own separate set of query, key, and value weights. Each weight matrix was of dimension  d x d/h  where h was the number of heads.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dY9PRtKlI_HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.d_head = self.d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(self.d_model, use_bias=False)\n",
        "    self.wk = tf.keras.layers.Dense(self.d_model, use_bias=False)\n",
        "    self.wv = tf.keras.layers.Dense(self.d_model, use_bias=False)\n",
        "\n",
        "    # Linear layer to generate the final output.\n",
        "    self.dense = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    split_inputs = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_head))\n",
        "    return tf.transpose(split_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def merge_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    merged_inputs = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    return tf.reshape(merged_inputs, (batch_size, -1, self.d_model))\n",
        "\n",
        "  def call(self, q, k, v, mask):\n",
        "    qs = self.wq(q)\n",
        "    ks = self.wk(k)\n",
        "    vs = self.wv(v)\n",
        "\n",
        "    qs = self.split_heads(qs)\n",
        "    ks = self.split_heads(ks)\n",
        "    vs = self.split_heads(vs)\n",
        "\n",
        "    output, attn_weights = scaled_dot_product_attention(qs, ks, vs, mask)\n",
        "    output = self.merge_heads(output)\n",
        "\n",
        "    return self.dense(output), attn_weights\n"
      ],
      "metadata": {
        "id": "oGtHXtzOT2fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward Neural Network"
      ],
      "metadata": {
        "id": "uid4XkJuqioD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward_network(d_model, hidden_dim):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "kQemwyfzOZfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode Block"
      ],
      "metadata": {
        "id": "Qtz8tJ9qq2VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "\n",
        "    self.mhsa1 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, target, training, decoder_mask):\n",
        "    mhsa_output1, attn_weights = self.mhsa1(target, target, target, decoder_mask)\n",
        "    mhsa_output1 = self.dropout1(mhsa_output1, training=training)\n",
        "    mhsa_output1 = self.layernorm1(mhsa_output1 + target)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output1)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    output = self.layernorm2(ffn_output + mhsa_output1)\n",
        "\n",
        "    return output, attn_weights\n"
      ],
      "metadata": {
        "id": "P4tSuI7sN323"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder with Mulitple Layers"
      ],
      "metadata": {
        "id": "NNxKmqrvq7GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "               max_seq_len, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(target_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.blocks = [DecoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_blocks)]\n",
        "\n",
        "  def call(self, input, training, decoder_mask):\n",
        "    token_embeds = self.token_embed(target)\n",
        "\n",
        "    # Generate position indices.\n",
        "    num_pos = input.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, input.shape)\n",
        "\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(x, training, decoder_mask)\n",
        "\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "c3NQtYApwsc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WoC4ksm1q6Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom loss function to remove effect of padding"
      ],
      "metadata": {
        "id": "3B_P9bRdrB4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(targets, logits):\n",
        "  ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  mask = tf.cast(tf.math.not_equal(targets, 0), tf.float32)\n",
        "  return ce_loss(targets, logits, sample_weight=mask)"
      ],
      "metadata": {
        "id": "8_Lyl3NmIXLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(tf.keras.Model):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "               max_input_len, dropout_rate=0.1):\n",
        "    super(GPT2Model, self).__init__()\n",
        "\n",
        "    self.decoder = Decoder(num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "                           max_input_len, dropout_rate)\n",
        "\n",
        "    # The final dense layer to generate logits from the model output.\n",
        "    self.output_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      loss = 0.\n",
        "\n",
        "      decoder_input_seq, targets = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "\n",
        "        dec_padding_mask = tf.cast(tf.math.not_equal(decoder_input_seq, 0), tf.float32)\n",
        "        dec_padding_mask = dec_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "        target_input_seq_len = len(decoder_input_seq[0])\n",
        "        look_ahead_mask = tf.linalg.band_part(tf.ones((target_input_seq_len,\n",
        "                                               target_input_seq_len)), -1, 0)\n",
        "        dec_mask = tf.minimum(dec_padding_mask, look_ahead_mask)\n",
        "\n",
        "        logits, _ = self.decoder(decoder_input_seq, True, dec_mask)\n",
        "        logits =   self.output_layer(logits)\n",
        "        loss += self.loss(targets, logits)\n",
        "\n",
        "      # Update the parameters and the optimizer\n",
        "      variables = self.decoder.trainable_variables\n",
        "      gradients = tape.gradient(loss, variables)\n",
        "      self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "      return {'loss': loss}\n",
        "\n",
        "  def call(self, target, training):\n",
        "    logits, _ = self.decoder(target, True, None)\n",
        "    logits =   self.output_layer(logits)\n",
        "    return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "scKF_bH_IuNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTModel(\n",
        "    num_blocks = 6,\n",
        "    d_model = 512,\n",
        "    num_heads = 4,\n",
        "    hidden_dim = 1024,\n",
        "    target_vocab_size = tokenizer.get_vocab_size(),\n",
        "    max_input_len = 256)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss=loss_func, run_eagerly=True)"
      ],
      "metadata": {
        "id": "0ebrfAFGh7yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "model.fit(tf_dataset, epochs=epochs)"
      ],
      "metadata": {
        "id": "kfVFfuLcOUtk",
        "outputId": "907e43d6-ce17-4ea7-8228-ef560fa0eec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "    847/Unknown - 582s 668ms/step - loss: 6.1027"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4duCI12SIO3Q"
      }
    }
  ]
}