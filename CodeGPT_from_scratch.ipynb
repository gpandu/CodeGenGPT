{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmXLbCm8oLD0lvOIl+EyWU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gpandu/CodeGenGPT/blob/main/CodeGPT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization:\n",
        "\n",
        "\n",
        "\n",
        "*   Subword Tokenization : Keep frequent words and break rearer words into subwords\n",
        "*   A statastical Alogrothm learns how to do this based on corpus.\n",
        "\n",
        "> Ex: Listeria ---> \"more\" , \"over\"\n",
        "\n",
        "> \"more\" and \"over\" are likely to be more frequent than moreover\n",
        "\n",
        "\n",
        "*   Tokenization has better chance of handling OOV words while decreasing the size of the overall dictionary.   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rHXniZJHcUYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "xS4gDfBKzOSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84bd19a-94de-4d1f-9cf1-0edd044799d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: huggingface_hub<0.17,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.7.22)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "tokenizer = Tokenizer(models.BPE())"
      ],
      "metadata": {
        "id": "kg8MrYtYNyZ9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "MbIllrfP6aHL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load code dataset\n",
        "raw_dataset = load_dataset(\"code_search_net\", \"python\")"
      ],
      "metadata": {
        "id": "G5lDmsDzXyd_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset"
      ],
      "metadata": {
        "id": "FNmrJRXKFin1",
        "outputId": "9f495865-558f-4681-90d8-f2aa38cccba5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
              "        num_rows: 412178\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
              "        num_rows: 22176\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
              "        num_rows: 23107\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "    dataset = raw_dataset[\"train\"]\n",
        "    for start_idx in range(0, len(dataset), 1000):\n",
        "        samples = dataset[start_idx : start_idx + 1000]\n",
        "        yield samples[\"whole_func_string\"]\n"
      ],
      "metadata": {
        "id": "z2N_vWvFeKwT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(get_training_corpus())\n",
        "next(iterator)[0]"
      ],
      "metadata": {
        "id": "j62xMmKOv1sl",
        "outputId": "857a3e39-33e3-42b4-d145-ffe9b363d0d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def export_ruptures_csv(ekey, dstore):\\n    \"\"\"\\n    :param ekey: export key, i.e. a pair (datastore key, fmt)\\n    :param dstore: datastore object\\n    \"\"\"\\n    oq = dstore[\\'oqparam\\']\\n    if \\'scenario\\' in oq.calculation_mode:\\n        return []\\n    dest = dstore.export_path(\\'ruptures.csv\\')\\n    header = (\\'rupid multiplicity mag centroid_lon centroid_lat \\'\\n              \\'centroid_depth trt strike dip rake boundary\\').split()\\n    rows = []\\n    for rgetter in gen_rupture_getters(dstore):\\n        rups = rgetter.get_ruptures()\\n        rup_data = calc.RuptureData(rgetter.trt, rgetter.rlzs_by_gsim)\\n        for r in rup_data.to_array(rups):\\n            rows.append(\\n                (r[\\'rup_id\\'], r[\\'multiplicity\\'], r[\\'mag\\'],\\n                 r[\\'lon\\'], r[\\'lat\\'], r[\\'depth\\'],\\n                 rgetter.trt, r[\\'strike\\'], r[\\'dip\\'], r[\\'rake\\'],\\n                 r[\\'boundary\\']))\\n    rows.sort()  # by rupture serial\\n    comment = \\'investigation_time=%s, ses_per_logic_tree_path=%s\\' % (\\n        oq.investigation_time, oq.ses_per_logic_tree_path)\\n    writers.write_csv(dest, rows, header=header, sep=\\'\\\\t\\', comment=comment)\\n    return [dest]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iterator)[0]"
      ],
      "metadata": {
        "id": "QdlVZqVXv8gm",
        "outputId": "b584703d-cdf3-4f77-fea8-9957a484ae42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def hmap_dt(self):  # used for CSV export\\n        \"\"\"\\n        :returns: a composite dtype (imt, poe)\\n        \"\"\"\\n        return numpy.dtype([(\\'%s-%s\\' % (imt, poe), F32)\\n                            for imt in self.imtls for poe in self.poes])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n"
      ],
      "metadata": {
        "id": "JuniW9ce_UTE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ],
      "metadata": {
        "id": "VJjwQcfI_Dhj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dataset[\"train\"][10][\"func_code_string\"]"
      ],
      "metadata": {
        "id": "XqjG1OFKF8VJ",
        "outputId": "921dd902-8853-4f64-d4a7-0b6199492efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\\n        \"\"\"\\n        See :meth:`superclass method\\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\\n        for spec of input and result values.\\n        \"\"\"\\n        # extracting dictionary of coefficients specific to required\\n        # intensity measure type.\\n        C = self.COEFFS[imt]\\n        if isinstance(imt, PGA):\\n            imt_per = 0.0\\n        else:\\n            imt_per = imt.period\\n        # Fix site parameters for consistent dS2S application.\\n        sites.vs30 = np.array([250])\\n        sites.z1pt0 = np.array([330])\\n        # intensity on a reference soil is used for both mean\\n        # and stddev calculations.\\n        ln_y_ref = self._get_ln_y_ref(rup, dists, C)\\n        # exp1 and exp2 are parts of eq. 7\\n        exp1 = np.exp(C[\\'phi3\\'] * (sites.vs30.clip(-np.inf, 1130) - 360))\\n        exp2 = np.exp(C[\\'phi3\\'] * (1130 - 360))\\n        # v1 is the period dependent site term. The Vs30 above which, the\\n        # amplification is constant\\n        v1 = self._get_v1(imt)\\n        # Get log-mean from regular unadjusted model\\n        b13a_mean = self._get_mean(sites, C, ln_y_ref, exp1, exp2, v1)\\n        # Adjust mean and standard deviation\\n        mean = b13a_mean + self._get_dL2L(imt_per) + self._get_dS2S(imt_per)\\n        mean += convert_to_LHC(imt)\\n        stddevs = self._get_adjusted_stddevs(sites, rup, C, stddev_types,\\n                                             ln_y_ref, exp1, exp2, imt_per)\\n\\n        return mean, stddevs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = '''def add_numbers(a, b):\n",
        "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
        "    return a + b'''\n",
        "\n",
        "tokenizer.enable_padding(length=256, pad_id = 0, pad_token = \"<|endoftext|>\")\n",
        "tokenizer.enable_truncation(max_length=256)\n",
        "encoding = tokenizer.encode(raw_dataset[\"train\"][1][\"whole_func_string\"])\n",
        "print(encoding.ids)\n",
        "print(encoding.attention_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLk7DRb2J0QU",
        "outputId": "87ba9c56-c695-45b2-ea0e-78b5599ce84c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[295, 4354, 63, 72, 6035, 63, 3170, 8, 368, 12, 1761, 12, 2840, 4475, 12, 960, 12, 3184, 274, 232, 290, 232, 14210, 256, 9081, 6778, 311, 256, 868, 2899, 1597, 1202, 8446, 14, 312, 310, 394, 521, 26, 819, 63, 369, 350, 4354, 63, 369, 232, 310, 394, 1761, 26, 455, 311, 256, 10245, 480, 232, 310, 394, 2840, 4475, 26, 2840, 2303, 232, 310, 394, 960, 26, 231, 11512, 960, 311, 1719, 366, 923, 63, 3112, 232, 310, 394, 3184, 26, 3184, 292, 811, 442, 1608, 311, 256, 10245, 8446, 480, 232, 290, 232, 12493, 233, 4705, 14, 11080, 63, 7194, 8, 2833, 4475, 12, 960, 9, 232, 2968, 501, 14, 1025, 63, 3170, 8, 2603, 12, 12493, 12, 3184, 29, 2124, 9, 232, 302, 404, 2603, 61, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "def generate_train_data():\n",
        "  tokenizer.enable_padding(length=257, pad_id = 0, pad_token = \"<|endoftext|>\")\n",
        "  tokenizer.enable_truncation(max_length=257)\n",
        "  decoder_inputs = []\n",
        "  decoder_targets = []\n",
        "  dataset = raw_dataset[\"train\"]\n",
        "  for start_idx in range(0, len(dataset), batch_size):\n",
        "      samples = dataset[start_idx : start_idx + batch_size]\n",
        "      seqs = tokenizer.encode_batch(samples[\"whole_func_string\"])\n",
        "      decoder_inputs = [seq.ids[:-1] for seq in seqs] # Drop the last token in the sentence.\n",
        "      decoder_targets = [seq.ids[1:] for seq in seqs]  # Drop the first token in the sentence.\n",
        "      yield decoder_inputs, decoder_targets\n"
      ],
      "metadata": {
        "id": "iRqM-3Bn2KTA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf_dataset = raw_dataset[\"train\"].to_tf_dataset(batch_size = 100, columns = ['whole_func_string'])\n",
        "#tf_dataset"
      ],
      "metadata": {
        "id": "9NBwGTRI4mFr",
        "outputId": "38736a9a-ba65-498d-ef1a-6cc81e846d1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "decoder_inputs, decoder_targets = next(iter(generate_train_data()))\n",
        "\n",
        "print(decoder_inputs[80])\n",
        "decoder_inputs, decoder_targets = next(iter(generate_train_data()))\n",
        "print(decoder_inputs[80])\n"
      ],
      "metadata": {
        "id": "7vvqGYjE2-y4",
        "outputId": "c7eb99f5-d319-432d-b45c-28a4cf0fa9b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[295, 627, 52, 18, 48, 13069, 8, 1317, 274, 232, 290, 232, 10197, 231, 19536, 238, 7287, 311, 231, 868, 7335, 3995, 14, 312, 310, 394, 11348, 26, 310, 692, 2310, 4535, 407, 5595, 64, 232, 310, 780, 26, 310, 692, 2310, 46, 395, 238, 48, 13069, 64, 312, 20346, 301, 497, 23110, 8516, 3097, 232, 572, 6313, 14, 77, 659, 1651, 1193, 3883, 14, 2936, 73, 14, 240, 299, 72, 320, 14, 11016, 15, 23701, 15, 338, 619, 12012, 15, 15678, 2561, 15, 15678, 2561, 14, 1825, 7564, 232, 4547, 585, 8898, 89, 627, 889, 65, 336, 350, 690, 279, 347, 4512, 12012, 14, 232, 290, 232, 308, 68, 12, 486, 9, 233, 635, 14, 5908, 14, 12657, 8, 1317, 14, 1317, 9, 232, 523, 233, 635, 14, 783, 929, 68, 59, 17, 547, 270, 59, 16, 547, 270, 59, 18, 4211, 232, 641, 233, 635, 14, 783, 5773, 86, 59, 17, 12, 396, 547, 415, 86, 59, 17, 12, 443, 547, 415, 86, 59, 17, 12, 554, 4936, 1797, 404, 86, 59, 18, 12, 396, 547, 415, 86, 59, 18, 12, 443, 547, 415, 86, 59, 18, 12, 554, 4936, 1797, 4866, 86, 59, 16, 12, 396, 547, 486, 59, 16, 12, 443, 547, 486, 59, 16, 12, 554, 23779, 232, 410, 5309, 233, 523, 14, 10535, 323, 232, 410, 6352, 233, 523, 14, 13410, 323, 232, 393, 37, 233, 308, 54, 1853, 410, 5309, 61, 382, 641, 1853, 410, 6352, 535, 823, 635, 14, 3429, 8, 18, 14, 16, 9, 232, 9893, 233]\n",
            "[295, 627, 52, 18, 48, 13069, 8, 1317, 274, 232, 290, 232, 10197, 231, 19536, 238, 7287, 311, 231, 868, 7335, 3995, 14, 312, 310, 394, 11348, 26, 310, 692, 2310, 4535, 407, 5595, 64, 232, 310, 780, 26, 310, 692, 2310, 46, 395, 238, 48, 13069, 64, 312, 20346, 301, 497, 23110, 8516, 3097, 232, 572, 6313, 14, 77, 659, 1651, 1193, 3883, 14, 2936, 73, 14, 240, 299, 72, 320, 14, 11016, 15, 23701, 15, 338, 619, 12012, 15, 15678, 2561, 15, 15678, 2561, 14, 1825, 7564, 232, 4547, 585, 8898, 89, 627, 889, 65, 336, 350, 690, 279, 347, 4512, 12012, 14, 232, 290, 232, 308, 68, 12, 486, 9, 233, 635, 14, 5908, 14, 12657, 8, 1317, 14, 1317, 9, 232, 523, 233, 635, 14, 783, 929, 68, 59, 17, 547, 270, 59, 16, 547, 270, 59, 18, 4211, 232, 641, 233, 635, 14, 783, 5773, 86, 59, 17, 12, 396, 547, 415, 86, 59, 17, 12, 443, 547, 415, 86, 59, 17, 12, 554, 4936, 1797, 404, 86, 59, 18, 12, 396, 547, 415, 86, 59, 18, 12, 443, 547, 415, 86, 59, 18, 12, 554, 4936, 1797, 4866, 86, 59, 16, 12, 396, 547, 486, 59, 16, 12, 443, 547, 486, 59, 16, 12, 554, 23779, 232, 410, 5309, 233, 523, 14, 10535, 323, 232, 410, 6352, 233, 523, 14, 13410, 323, 232, 393, 37, 233, 308, 54, 1853, 410, 5309, 61, 382, 641, 1853, 410, 6352, 535, 823, 635, 14, 3429, 8, 18, 14, 16, 9, 232, 9893, 233]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf_dataset = tf.data.Dataset.from_tensor_slices((decoder_inputs, decoder_targets)).batch(10, drop_remainder=True)\n",
        "\n",
        "tf_dataset = tf.data.Dataset.from_generator(generate_train_data, output_types=(tf.int32, tf.int32), output_shapes=(tf.TensorShape([50,256]), tf.TensorShape([50,256])))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ysecpmr7uW_X"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter(tf_dataset)\n",
        "a,b = next(iterator)\n",
        "print(a.shape)"
      ],
      "metadata": {
        "id": "ShQELQ-CJbiW",
        "outputId": "b5d2c4cb-c3db-4453-cfa1-a80d9752e026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iterator)[0]"
      ],
      "metadata": {
        "id": "zJuJA3ryJv53",
        "outputId": "c8640795-67d2-4120-d5c0-85a951f94236",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 256), dtype=int32, numpy=\n",
              "array([[ 295,  549,   63, ...,  357, 3665,   19],\n",
              "       [ 295,  409, 8485, ...,    0,    0,    0],\n",
              "       [ 295,  409,  322, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [ 295, 1339,    8, ...,    0,    0,    0],\n",
              "       [ 295, 2274,   63, ..., 3921,   63,   66],\n",
              "       [ 295,  549,   63, ...,    0,    0,    0]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Head Attention\n",
        "\n",
        "\n",
        "\n",
        "*   Each Attention head performs Scaled Dot Product Self-Attention operation where given Keys, Query and Values, the return matrix of values given by below operation.\n",
        "\n",
        "        Attention(Q,K,V) = softmax((Q*Transpose(K))/sqrt(d))*V\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WROQaaygJlOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  key_dims = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  scaled_scores = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(key_dims)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
        "\n",
        "  softmax = tf.keras.layers.Softmax()\n",
        "  weights = softmax(scaled_scores)\n",
        "  return tf.matmul(weights, value), weights\n",
        "\n"
      ],
      "metadata": {
        "id": "Q51xPbdZKdc5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating queries, keys, and values for multiple heads.**\n",
        "\n",
        "> Now that we have a way to calculate self-attention, let's actually generate the input queries, keys, and values for multiple heads.\n",
        "\n",
        ">  each attention head had its own separate set of query, key, and value weights. Each weight matrix was of dimension  d x d/h  where h was the number of heads.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dY9PRtKlI_HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.d_head = self.d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(self.d_model, use_bias=False)\n",
        "    self.wk = tf.keras.layers.Dense(self.d_model, use_bias=False)\n",
        "    self.wv = tf.keras.layers.Dense(self.d_model, use_bias=False)\n",
        "\n",
        "    # Linear layer to generate the final output.\n",
        "    self.dense = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    split_inputs = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_head))\n",
        "    return tf.transpose(split_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def merge_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    merged_inputs = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    return tf.reshape(merged_inputs, (batch_size, -1, self.d_model))\n",
        "\n",
        "  def call(self, q, k, v, mask):\n",
        "    qs = self.wq(q)\n",
        "    ks = self.wk(k)\n",
        "    vs = self.wv(v)\n",
        "\n",
        "    qs = self.split_heads(qs)\n",
        "    ks = self.split_heads(ks)\n",
        "    vs = self.split_heads(vs)\n",
        "\n",
        "    output, attn_weights = scaled_dot_product_attention(qs, ks, vs, mask)\n",
        "    output = self.merge_heads(output)\n",
        "\n",
        "    return self.dense(output), attn_weights\n"
      ],
      "metadata": {
        "id": "oGtHXtzOT2fU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward Neural Network"
      ],
      "metadata": {
        "id": "uid4XkJuqioD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward_network(d_model, hidden_dim):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "kQemwyfzOZfC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode Block"
      ],
      "metadata": {
        "id": "Qtz8tJ9qq2VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "\n",
        "    self.mhsa1 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  # Note the decoder block takes two masks. One for the first MHSA, another\n",
        "  # for the second MHSA.\n",
        "  def call(self, target, training, decoder_mask):\n",
        "    mhsa_output1, attn_weights = self.mhsa1(target, target, target, decoder_mask)\n",
        "    mhsa_output1 = self.dropout1(mhsa_output1, training=training)\n",
        "    mhsa_output1 = self.layernorm1(mhsa_output1 + target)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output1)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    output = self.layernorm2(ffn_output + mhsa_output1)\n",
        "\n",
        "    return output, attn_weights\n"
      ],
      "metadata": {
        "id": "P4tSuI7sN323"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder with Mulitple Layers"
      ],
      "metadata": {
        "id": "NNxKmqrvq7GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "               max_seq_len, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(target_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.blocks = [DecoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_blocks)]\n",
        "\n",
        "  def call(self, target, training, decoder_mask):\n",
        "    token_embeds = self.token_embed(target)\n",
        "\n",
        "    # Generate position indices.\n",
        "    num_pos = target.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, target.shape)\n",
        "\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(x, training, decoder_mask)\n",
        "\n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "c3NQtYApwsc5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WoC4ksm1q6Kp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom loss function to remove effect of padding"
      ],
      "metadata": {
        "id": "3B_P9bRdrB4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(targets, logits):\n",
        "  ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  mask = tf.cast(tf.math.not_equal(targets, 0), tf.float32)\n",
        "\n",
        "  return ce_loss(targets, logits, sample_weight=mask)"
      ],
      "metadata": {
        "id": "8_Lyl3NmIXLk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Model(tf.keras.Model):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "               max_input_len, dropout_rate=0.1):\n",
        "    super(GPT2Model, self).__init__()\n",
        "\n",
        "    self.decoder = Decoder(num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,\n",
        "                           max_input_len, dropout_rate)\n",
        "    # The final dense layer to generate logits from the decoder output.\n",
        "    self.output_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      loss = 0.\n",
        "\n",
        "      decoder_input_seq, targets = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "\n",
        "        dec_padding_mask = tf.cast(tf.math.not_equal(decoder_input_seq, 0), tf.float32)\n",
        "        dec_padding_mask = dec_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "        target_input_seq_len = len(decoder_input_seq[0])\n",
        "        look_ahead_mask = tf.linalg.band_part(tf.ones((target_input_seq_len,\n",
        "                                               target_input_seq_len)), -1, 0)\n",
        "        dec_mask = tf.minimum(dec_padding_mask, look_ahead_mask)\n",
        "\n",
        "        logits, _ = self.decoder(decoder_input_seq, True, dec_mask)\n",
        "        logits =   self.output_layer(logits)\n",
        "        # The loss is now accumulated through the whole batch\n",
        "        loss += self.loss(targets, logits)\n",
        "\n",
        "      # Update the parameters and the optimizer\n",
        "      variables = self.decoder.trainable_variables\n",
        "      gradients = tape.gradient(loss, variables)\n",
        "      self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "      return {'loss': loss}\n",
        "\n",
        "  def call(self, target, training):\n",
        "    logits, _ = self.decoder(target, True, None)\n",
        "    logits =   self.output_layer(logits)\n",
        "    return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "scKF_bH_IuNO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2Model(\n",
        "    num_blocks = 6,\n",
        "    d_model = 512,\n",
        "    num_heads = 4,\n",
        "    hidden_dim = 1024,\n",
        "    target_vocab_size = tokenizer.get_vocab_size(),\n",
        "    max_input_len = 256)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss=loss_func, run_eagerly=True)"
      ],
      "metadata": {
        "id": "0ebrfAFGh7yf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "model.fit(tf_dataset, epochs=epochs)"
      ],
      "metadata": {
        "id": "kfVFfuLcOUtk",
        "outputId": "907e43d6-ce17-4ea7-8228-ef560fa0eec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "    847/Unknown - 582s 668ms/step - loss: 6.1027"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4duCI12SIO3Q"
      }
    }
  ]
}